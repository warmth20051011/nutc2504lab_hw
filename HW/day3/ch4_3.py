from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tools import tool


llm = ChatOpenAI(
    base_url="https://ws-05.huannago.com/v1",
    api_key="vllm-token",
    model="Qwen/Qwen3-VL-8B-Instruct"
)


@tool
def generate_tech_summary(article_content: str) -> str:
    """å°‡ç§‘æŠ€æ–‡ç« å…§å®¹æ­¸ç´å‡º 3 å€‹é‡é»æ‘˜è¦"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç§‘æŠ€æ–‡ç« ç·¨è¼¯ï¼Œè«‹å°‡ä½¿ç”¨è€…æä¾›çš„æ–‡ç« å…§å®¹ï¼Œæ­¸ç´å‡º 3 å€‹é‡é»ï¼Œä¸¦ä»¥ç¹é«”ä¸­æ–‡æ¢åˆ—å¼è¼¸å‡ºã€‚"),
        ("human", "{text}")
    ])

    chain = prompt | llm | StrOutputParser()
    return chain.invoke({"text": article_content})



llm_with_tools = llm.bind_tools([generate_tech_summary])

router_prompt = ChatPromptTemplate.from_messages([
    ("user", "{input}")
])

while True:
    user_input = input("User: ")
    if user_input.lower() in ["exit", "q"]:
        print("Bye!")
        break

    chain = router_prompt | llm_with_tools
    ai_msg = chain.invoke({"input": user_input})

    if ai_msg.tool_calls:
        print("âœ… [æ±ºç­–] åˆ¤æ–·ç‚ºç§‘æŠ€æ–‡ç« ")
        tool_args = ai_msg.tool_calls[0]['args']
        final_result = generate_tech_summary.invoke(tool_args)
        print(f"ğŸ“ [åŸ·è¡Œçµæœ]:\n{final_result}")
    else:
        print("âŒ [æ±ºç­–] åˆ¤æ–·ç‚ºé–’èŠ/éç§‘æŠ€æ–‡ç« ï¼Œç›´æ¥å›è¦†ã€‚")
        print(f"ğŸ’¬ [AI å›æ‡‰]: {ai_msg.content}")

