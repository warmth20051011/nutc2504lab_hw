import os
from pathlib import Path
import requests

import pytesseract
import pdfplumber
from pdf2image import convert_from_path
from docx import Document
from PIL import Image

from openai import OpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct


# =========================
# åŸºæœ¬è¨­å®š
# =========================

BASE_DIR = Path(__file__).parent

EMBED_URL = "https://ws-04.wade0426.me/embed"

LLM_BASE_URL = "https://ws-06.huannago.com/v1"
LLM_MODEL = "gemma-3-27b-it"

client = OpenAI(
    base_url=LLM_BASE_URL,
    api_key="NoNeed"
)

CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
TOP_K = 5


# =========================
# IDP æ–‡ä»¶è®€å–
# =========================

def extract_pdf_text(path):
    texts = []
    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            t = page.extract_text()
            if t:
                texts.append(t)
    return "\n".join(texts)


def extract_pdf_ocr(path):
    images = convert_from_path(path, dpi=200)
    texts = []
    for img in images:
        texts.append(
            pytesseract.image_to_string(img, lang="chi_tra+eng")
        )
    return "\n".join(texts)


def extract_image(path):
    img = Image.open(path)
    return pytesseract.image_to_string(img, lang="chi_tra+eng")


def extract_docx(path):
    doc = Document(path)
    return "\n".join(
        [p.text for p in doc.paragraphs if p.text.strip()]
    )


def load_documents():
    docs = {}

    for f in ["1.pdf", "2.pdf", "3.pdf", "4.png", "5.docx"]:

        path = BASE_DIR / f
        if not path.exists():
            continue

        print(f"ğŸ“„ è®€å– {f}")

        if f.endswith(".pdf"):
            text = extract_pdf_text(path)

            # å¦‚æœ PDF å¹¾ä¹æ²’æŠ“åˆ°å­— â†’ æ”¹ OCR
            if len(text.strip()) < 100:
                print("   â†’ æ–‡å­—å¤ªå°‘ï¼Œæ”¹ç”¨ OCR")
                text = extract_pdf_ocr(path)

        elif f.endswith(".png"):
            text = extract_image(path)

        elif f.endswith(".docx"):
            text = extract_docx(path)

        else:
            text = ""

        print(f"   â†’ OK ({len(text)} chars)")
        docs[f] = text

    return docs


# =========================
# åˆ‡å¡Š
# =========================

def split_text(text, source):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )

    chunks = splitter.split_text(text)

    return [
        {"text": c, "source": source}
        for c in chunks
    ]


# =========================
# å‘é‡åµŒå…¥
# =========================

def embed(texts):

    r = requests.post(
        EMBED_URL,
        json={
            "texts": texts,
            "task_description": "qa",
            "normalize": True
        },
        timeout=60
    )

    return r.json()["embeddings"]


# =========================
# å»ºç«‹å‘é‡åº«
# =========================

def build_index(chunks):

    print("ğŸ“¦ å»ºç«‹å‘é‡åº«...")

    client_q = QdrantClient(":memory:")

    test_vec = embed(["test"])[0]
    dim = len(test_vec)

    client_q.create_collection(
        collection_name="docs",
        vectors_config=VectorParams(
            size=dim,
            distance=Distance.COSINE
        )
    )

    vectors = embed([c["text"] for c in chunks])

    points = [
        PointStruct(
            id=i,
            vector=v,
            payload={
                "text": chunks[i]["text"],
                "source": chunks[i]["source"]
            }
        )
        for i, v in enumerate(vectors)
    ]

    client_q.upsert("docs", points)

    print("âœ… å‘é‡åº«å»ºç«‹å®Œæˆ")

    return client_q


# =========================
# æœå°‹
# =========================

def search(client_q, query):

    q_vec = embed([query])[0]

    res = client_q.query_points(
        "docs",
        query=q_vec,
        limit=TOP_K
    )

    return res.points


# =========================
# ç”Ÿæˆå›ç­”
# =========================

def generate_answer(query, contexts):

    ctx = "\n".join(
        [c.payload["text"] for c in contexts]
    )

    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­æ–‡ä»¶å•ç­”åŠ©æ‰‹ï¼Œåƒ…èƒ½æ ¹æ“šè³‡æ–™å›ç­”ã€‚"
        },
        {
            "role": "user",
            "content": f"è³‡æ–™:\n{ctx}\n\nå•é¡Œ:{query}"
        }
    ]

    resp = client.chat.completions.create(
        model=LLM_MODEL,
        messages=messages,
        temperature=0
    )

    return resp.choices[0].message.content


# =========================
# ä¸»ç¨‹å¼
# =========================

def main():

    print("ğŸ“‚ è¼‰å…¥æ–‡ä»¶ä¸­...\n")

    docs = load_documents()

    print("\nâœ‚ï¸ åˆ‡å¡Šä¸­...")

    all_chunks = []

    for name, text in docs.items():
        all_chunks += split_text(text, name)

    print(f"   â†’ å…± {len(all_chunks)} å€‹ chunks\n")

    qdrant_client = build_index(all_chunks)

    print("\nğŸš€ æ–‡ä»¶å•ç­”ç³»çµ±å·²å•Ÿå‹•\n")

    while True:

        query = input("è«‹è¼¸å…¥å•é¡Œï¼ˆè¼¸å…¥ exit é›¢é–‹ï¼‰ï¼š")

        if query.lower() == "exit":
            break

        if not query.strip():
            continue

        contexts = search(qdrant_client, query)

        answer = generate_answer(query, contexts)

        sources = list(
            set([c.payload["source"] for c in contexts])
        )

        print("\n==============================")
        print("å›ç­”ï¼š\n")
        print(answer)
        print("\nğŸ“š ä¾†æºæ–‡ä»¶ï¼š", ", ".join(sources))
        print("==============================\n")


if __name__ == "__main__":
    main()
