import os
import re
import csv
import requests
from pathlib import Path

import pytesseract
import pdfplumber
from pdf2image import convert_from_path
from docx import Document

from openai import OpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

from deepeval.metrics import (
    FaithfulnessMetric,
    AnswerRelevancyMetric,
    ContextualRecallMetric,
    ContextualPrecisionMetric,
)
from deepeval.test_case import LLMTestCase
from deepeval.models import DeepEvalBaseLLM


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åŸºæœ¬è¨­å®š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BASE_DIR = Path(__file__).parent
EMBED_URL = "https://ws-04.wade0426.me/embed"

LLM_BASE_URL = "https://ws-06.huannago.com/v1"
LLM_MODEL = "gemma-3-27b-it"
LLM_API_KEY = "NoNeed"

client = OpenAI(base_url=LLM_BASE_URL, api_key=LLM_API_KEY)

CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
TOP_K = 5
MAX_SAMPLES = 5   # â­ é™åˆ¶å‰ 5 ç­†


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IDP æ–‡ä»¶æå–
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_pdf_text(path):
    texts = []
    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            t = page.extract_text()
            if t:
                texts.append(t)
    return "\n".join(texts)

def extract_pdf_ocr(path):
    images = convert_from_path(path, dpi=200)
    texts = []
    for img in images:
        texts.append(pytesseract.image_to_string(img, lang="chi_tra+eng"))
    return "\n".join(texts)

def extract_image(path):
    from PIL import Image
    img = Image.open(path)
    return pytesseract.image_to_string(img, lang="chi_tra+eng")

def extract_docx(path):
    doc = Document(path)
    return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])

def load_documents():
    docs = {}
    for f in ["1.pdf", "2.pdf", "3.pdf", "4.png", "5.docx"]:
        path = BASE_DIR / f
        if not path.exists():
            continue

        if f.endswith(".pdf"):
            text = extract_pdf_text(path)
            if len(text.strip()) < 100:
                text = extract_pdf_ocr(path)
        elif f.endswith(".png"):
            text = extract_image(path)
        elif f.endswith(".docx"):
            text = extract_docx(path)

        docs[f] = text
    return docs


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Injection åµæ¸¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PATTERNS = [
    r"ignore (previous|above) instructions",
    r"è«‹å¿½ç•¥.*æŒ‡ç¤º",
    r"ä½ æ˜¯ä¸€å€‹.*LLM",
    r"system prompt",
    r"jailbreak",
    r"do not follow.*rules",
    r"ä¸è¦éµå®ˆ.*è¦å‰‡",
    r"act as .*",
    r"developer mode",
]

def detect_injection(text):
    for p in PATTERNS:
        if re.search(p, text, re.IGNORECASE):
            return True
    return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def split_text(text, source):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )
    chunks = splitter.split_text(text)
    return [{"text": c, "source": source} for c in chunks]

def embed(texts):
    r = requests.post(
        EMBED_URL,
        json={"texts": texts, "task_description": "qa", "normalize": True},
        timeout=150
    )
    return r.json()["embeddings"]

def build_index(chunks):
    client_q = QdrantClient(":memory:")
    test_vec = embed(["test"])[0]
    dim = len(test_vec)

    client_q.create_collection(
        collection_name="docs",
        vectors_config=VectorParams(size=dim, distance=Distance.COSINE)
    )

    vectors = embed([c["text"] for c in chunks])

    points = [
        PointStruct(
            id=i,
            vector=v,
            payload={
                "text": chunks[i]["text"],
                "source": chunks[i]["source"]
            }
        )
        for i, v in enumerate(vectors)
    ]

    client_q.upsert("docs", points)
    return client_q

def search(client_q, query):
    q_vec = embed([query])[0]
    res = client_q.query_points("docs", query=q_vec, limit=TOP_K)
    return res.points

def generate_answer(query, contexts):
    ctx = "\n".join([c.payload["text"] for c in contexts])
    msg = [
        {"role": "system", "content": "æ ¹æ“šè³‡æ–™å›ç­”å•é¡Œï¼Œä¸å¯ç·¨é€ ã€‚"},
        {"role": "user", "content": f"è³‡æ–™:\n{ctx}\n\nå•é¡Œ:{query}"}
    ]
    resp = client.chat.completions.create(
        model=LLM_MODEL,
        messages=msg,
        temperature=0
    )
    return resp.choices[0].message.content


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DeepEval
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CustomLLM(DeepEvalBaseLLM):

    def load_model(self):
        return None

    def generate(self, prompt: str):
        resp = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        return resp.choices[0].message.content

    async def a_generate(self, prompt: str):
        return self.generate(prompt)

    def get_model_name(self):
        return LLM_MODEL


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»ç¨‹å¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():

    print("ğŸ“„ è¼‰å…¥æ–‡ä»¶...")
    docs = load_documents()

    print("ğŸ” æª¢æ¸¬ Injection...")
    clean_docs = {}
    for name, text in docs.items():
        if detect_injection(text):
            print(f"âŒ ç™¼ç¾æƒ¡æ„æç¤ºè©: {name} â†’ å‰ƒé™¤")
        else:
            clean_docs[name] = text

    print("âœ‚ï¸ åˆ‡å¡Š...")
    all_chunks = []
    for name, text in clean_docs.items():
        all_chunks += split_text(text, name)

    print("ğŸ“¦ å»ºç«‹å‘é‡åº«...")
    qdrant_client = build_index(all_chunks)

    qa_data = []
    with open(BASE_DIR / "questions_answer.csv", "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for i, row in enumerate(reader):
            if i >= MAX_SAMPLES:
                break
            qa_data.append(row)

    custom_llm = CustomLLM()

    metrics = [
        FaithfulnessMetric(model=custom_llm),
        AnswerRelevancyMetric(model=custom_llm),
        ContextualRecallMetric(model=custom_llm),
        ContextualPrecisionMetric(model=custom_llm),
    ]

    print("ğŸš€ åŸ·è¡Œ RAG + DeepEval...")

    for row in qa_data:

        query = row["questions"]
        contexts = search(qdrant_client, query)
        answer = generate_answer(query, contexts)

        test_case = LLMTestCase(
            input=query,
            actual_output=answer,
            expected_output=row["answer"],
            retrieval_context=[c.payload["text"] for c in contexts]
        )

        for m in metrics:
            m.measure(test_case)

        print("\n" + "=" * 60)
        print(f"Q: {query}")
        print(f"A: {answer}")
        print("Scores:")
        print("Faithfulness:", metrics[0].score)
        print("AnswerRelevancy:", metrics[1].score)
        print("ContextualRecall:", metrics[2].score)
        print("ContextualPrecision:", metrics[3].score)

    print("\nğŸ“„ ç”¢ç”Ÿ test_dataset.csv...")

    rows = []

    with open(BASE_DIR / "questions_answer.csv", "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for i, row in enumerate(reader):

            if i >= MAX_SAMPLES:
                break

            query = row["questions"]
            contexts = search(qdrant_client, query)
            answer = generate_answer(query, contexts)

            sources = list(set([c.payload["source"] for c in contexts]))

            rows.append({
                "q_id": row["id"],
                "questions": query,
                "answer": answer,
                "source": ",".join(sources)
            })

    with open(BASE_DIR / "test_dataset.csv", "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=["q_id", "questions", "answer", "source"]
        )
        writer.writeheader()
        writer.writerows(rows)

    print("âœ… test_dataset.csv ç”¢ç”Ÿå®Œæˆ")
    print("ğŸ‰ ä½œæ¥­å®Œæˆ")


if __name__ == "__main__":
    main()

