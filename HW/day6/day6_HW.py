import csv
import re
import numpy as np
from typing import List

from langchain_openai import ChatOpenAI
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from rank_bm25 import BM25Okapi

# ============================================================
# åŸºæœ¬è¨­å®š
# ============================================================

QA_PATH = "qa_data.txt"
QUESTION_PATH = "questions.csv"
OUTPUT_CSV = "day6_HW_questions.csv"

llm = ChatOpenAI(
    base_url="https://ws-02.wade0426.me/v1",
    api_key="",
    model="google/gemma-3-27b-it",
    temperature=0.1
)

embed_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# ============================================================
# è®€å– QA è³‡æ–™
# ============================================================

def is_question(line: str) -> bool:
    return (
        len(line) < 80
        and ("ï¼Ÿ" in line or "?" in line)
        and not line.startswith("ä¾†æº")
        and "ç™¼å¸ƒæ—¥æœŸ" not in line
    )

def load_qa(path: str):
    qa_pairs = []

    with open(path, encoding="utf-8") as f:
        lines = [l.strip() for l in f if l.strip()]

    current_q = None
    current_a = []

    for line in lines:
        if is_question(line):
            if current_q and current_a:
                qa_pairs.append({
                    "q": current_q,
                    "a": "\n".join(current_a)
                })
            current_q = line
            current_a = []
            continue

        if line.startswith("ä¾†æº") or "ç™¼å¸ƒæ—¥æœŸ" in line:
            continue

        if current_q:
            current_a.append(line)

    if current_q and current_a:
        qa_pairs.append({
            "q": current_q,
            "a": "\n".join(current_a)
        })

    return qa_pairs

# çœŸæ­£è¼‰å…¥ QA
qa_data = load_qa(QA_PATH)
print(f"ðŸ“„ è¼‰å…¥ QA ç­†æ•¸ï¼š{len(qa_data)}")

# ============================================================
# Hybrid Search å»ºç«‹
# ============================================================

corpus = [item["q"] + " " + item["a"] for item in qa_data]

# BM25 ä¿å‘½æª¢æŸ¥ï¼ˆä¸€å®šè¦æœ‰ï¼‰
if not corpus:
    raise ValueError("âŒ QA corpus ç‚ºç©ºï¼Œè«‹ç¢ºèª qa_data.txt æ˜¯å¦æœ‰å…§å®¹")

# BM25
tokenized = [doc.split() for doc in corpus]
bm25 = BM25Okapi(tokenized)

# Dense
corpus_embeddings = embed_model.encode(
    corpus, normalize_embeddings=True
)

# ============================================================
# Query Rewrite
# ============================================================

def rewrite_query(query: str, history: List[str]) -> str:
    prompt = f"""
ä½ æ˜¯ AI å®¢æœåŠ©ç†ï¼Œè«‹æ ¹æ“šå°è©±æ­·å²ï¼Œå°‡ä½¿ç”¨è€…å•é¡Œæ”¹å¯«æˆæ¸…æ¥šã€å®Œæ•´çš„æŸ¥è©¢å¥ã€‚

å°è©±æ­·å²ï¼š
{history}

ä½¿ç”¨è€…å•é¡Œï¼š
{query}

è«‹åªè¼¸å‡ºæ”¹å¯«å¾Œçš„å•é¡Œã€‚
"""
    return llm.invoke(prompt).content.strip()

# ============================================================
# Hybrid Search
# ============================================================

def hybrid_search(query: str, top_k=5):
    bm25_scores = bm25.get_scores(query.split())

    q_emb = embed_model.encode([query], normalize_embeddings=True)
    dense_scores = cosine_similarity(
        q_emb, corpus_embeddings
    )[0]

    scores = 0.5 * bm25_scores + 0.5 * dense_scores
    top_idx = np.argsort(scores)[::-1][:top_k]

    return [(i, corpus[i]) for i in top_idx]

# ============================================================
# Rerankï¼ˆLLMï¼‰
# ============================================================

def rerank(query: str, docs: List[str]) -> str:
    context = "\n\n".join(docs)

    prompt = f"""
ä½ æ˜¯æ°´å‹™å…¬å¸çš„ AI å®¢æœåŠ©ç†ï¼Œè«‹æ ¹æ“šä¸‹æ–¹æä¾›çš„è³‡æ–™å›žç­”å•é¡Œã€‚

è¦å‰‡ï¼š
1. **åªèƒ½æ ¹æ“šæä¾›çš„è³‡æ–™å›žç­”**
2. å¦‚æžœè³‡æ–™ã€Œæœ‰ç›¸é—œä½†ä¸å®Œå…¨ä¸€æ¨¡ä¸€æ¨£ã€ï¼Œè«‹ç”¨åˆç†æŽ¨è«–å›žç­”
3. å¦‚æžœçœŸçš„å®Œå…¨ç„¡é—œï¼Œæ‰å›žç­”ã€Œè³‡æ–™ä¸­æœªæåŠã€

ä½¿ç”¨è€…å•é¡Œï¼š
{query}

åƒè€ƒè³‡æ–™ï¼š
{context}

è«‹ç›´æŽ¥çµ¦å‡ºå®Œæ•´ã€è‡ªç„¶çš„å®¢æœå›žç­”ï¼Œä¸è¦æåˆ°ã€Œè³‡æ–™ä¸­æ²’æœ‰ã€é€™ç¨®æè¿°ã€‚
"""

    return llm.invoke(prompt).content.strip()


# ============================================================
# AI å®¢æœï¼ˆå¤šè¼ªï¼‰
# ============================================================

def chat():
    history = []

    while True:
        user_q = input("\nä½¿ç”¨è€…ï¼š").strip()
        if user_q.lower() in ["q", "quit"]:
            break

        rewritten = rewrite_query(user_q, history)
        hits = hybrid_search(rewritten)
        docs = [d for _, d in hits]
        answer = rerank(rewritten, docs)

        history.append(f"ä½¿ç”¨è€…ï¼š{user_q}")
        history.append(f"åŠ©ç†ï¼š{answer}")

        print("\nðŸ¤– åŠ©ç†ï¼š", answer)

# ============================================================
# æ‰¹æ¬¡å›žç­”ï¼ˆç”¢ CSV çµ¦ DeepEvalï¼‰
# ============================================================

def batch_answer():
    rows = []

    with open(QUESTION_PATH, encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rewritten = rewrite_query(row["questions"], [])
            hits = hybrid_search(rewritten)
            docs = [d for _, d in hits]
            answer = rerank(rewritten, docs)

            rows.append({
                "q_id": row["q_id"],
                "questions": row["questions"],
                "answer": answer,
                "Faithfulness": "",
                "Answer_Relevancy": "",
                "Contextual_Recall": "",
                "Contextual_Precision": "",
                "Contextual_Relevancy": ""
            })

    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=rows[0].keys())
        writer.writeheader()
        writer.writerows(rows)

    print("âœ… day6_HW_questions.csv å·²ç”¢ç”Ÿ")

# ============================================================
# Main
# ============================================================

if __name__ == "__main__":
    mode = input("1: äº’å‹•å®¢æœ  2: ç”¢ CSV > ")
    if mode == "1":
        chat()
    else:
        batch_answer()

