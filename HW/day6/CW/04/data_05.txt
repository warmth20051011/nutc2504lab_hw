Google更新裝置端推論框架LiteRT，宣布在Google I/O 2025預告的進階硬體加速能力，已正式納入LiteRT產品堆疊並對開發者開放。這次更新把GPU與NPU的加速流程補齊，其中GPU支援從I/O 2025當時先在Android導入的路徑，擴大到Android、iOS、macOS、Windows、Linux與Web，讓裝置端AI推論在行動端、桌面與網頁之間更一致。

LiteRT技術堆疊從TensorFlow Lite的基礎往前推進，過去TensorFlow Lite主要服務傳統機器學習推論，而LiteRT的定位則是接手新一代裝置端AI需求，包含更廣泛的硬體加速與跨平臺部署。

LiteRT的GPU加速支援範圍涵蓋Android、iOS、macOS、Windows、Linux與Web，並透過ML Drift這套下一代GPU引擎，串接OpenCL、OpenGL、Metal與WebGPU等後端。在Android裝置上，LiteRT會在可用時優先採用OpenCL以取得較高效能，必要時退回OpenGL，其他平臺則改以各自的GPU後端，例如macOS使用Metal，Windows與Linux使用WebGPU。Google表示，在多種模型的平均情境下，LiteRT的GPU效能平均約比既有的TensorFlow Lite GPU委派快1.4倍。

LiteRT的效能改善，來自推論從輸入到輸出的整體等待時間縮短，Google讓裝置端推論更少依賴CPU執行額外的等待與資料處理，並降低資料在不同硬體之間搬移時造成的延遲。

Google點出目前NPU的挑戰不是單一硬體效能，而是生態系碎片化。面對不同晶片平臺與各家供應商工具鏈差異，開發者通常得用多套方式，才能把同一個模型部署到不同裝置，導致維運成本上升。LiteRT的訴求是把差異整合進同一套機制，讓開發者以較一致的方法啟用NPU加速，並在裝置不支援或條件不足時，仍能自動改用GPU或CPU維持可用性。

在NPU部署流程上，Google把工作流簡化為三個步驟，包含可選的AOT預先編譯、在Android上可搭配Google Play for On-device AI將模型與執行階段交付到相容裝置，以及由LiteRT執行環境負責啟用NPU委派並在條件不足時回退到GPU或CPU。同時，LiteRT提供AOT與裝置端JIT兩種編譯策略，讓開發者在啟動速度與首次執行成本之間做取捨。

LiteRT維持以.tflite格式作為跨平臺部署的共同基礎，開發者可利用PyTorch、TensorFlow與JAX等常見訓練框架轉換模型，讓不同來源的模型都能接上同一套裝置端推論與硬體加速能力，降低因訓練框架不同而造成的部署分歧。